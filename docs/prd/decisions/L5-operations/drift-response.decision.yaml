decision_id: drift_response
title: "Drift Response"
description: >
  How the system responds to detected data or model drift. Automated
  retraining triggers a new training run; alert-human notifies developers
  for manual assessment; gradual rollback reverts to previous model
  versions. Critical for maintaining model reliability over time in
  medical imaging where distribution shifts can be subtle. Zhang et al.
  (2026) introduce Prediction-Powered Risk Monitoring (PPRM) with formal
  false alarm guarantees for semi-supervised drift detection. Subasri et al.
  (2023) demonstrate unidirectional data shifts across hospitals and evaluate
  transfer learning vs. continual learning remediation. Krishnan et al.
  (2022) provide the CyclOps toolkit for healthcare ML auditing, drift
  detection, and fairness evaluation.

decision_level: L5_operations
status: active
last_updated: 2026-02-24

options:
  - option_id: automated_retrain
    title: "Automated Retrain"
    description: >
      Automatically trigger retraining pipeline when drift exceeds
      thresholds. Requires robust data validation, evaluation gates,
      and model governance. Most mature but highest implementation cost.
    prior_probability: 0.30
    status: viable
    implementation_status: not_started
    constraints:
      - "Requires validated retraining pipeline"
      - "Evaluation gates to prevent deploying worse models"
      - "Data quality checks before retraining"
    complements:
      - "retraining_trigger.drift_based"
      - "model_governance.full_registry"

  - option_id: alert_human
    title: "Alert Human for Assessment"
    description: >
      Notify developers/clinicians when drift is detected. Human reviews
      metrics and decides on response. Current partial implementation
      via Evidently reports and whylogs profiles.
    prior_probability: 0.30
    status: resolved
    implementation_status: partial
    complements:
      - "monitoring_stack.prometheus_grafana"
      - "model_governance.mlflow_tags"

  - option_id: gradual_rollback
    title: "Gradual Rollback"
    description: >
      Automatically shift traffic to previous model version using
      canary deployment or shadow scoring. Requires model versioning
      and traffic management infrastructure.
    prior_probability: 0.20
    status: viable
    implementation_status: not_started
    constraints:
      - "Requires serving infrastructure for multi-model traffic"
      - "Model versioning and registry"
      - "Rollback criteria definition"

  - option_id: none
    title: "No Drift Response"
    description: >
      No automated drift response. Rely on periodic manual model
      evaluation. Current effective state for development.
    prior_probability: 0.20
    status: viable
    implementation_status: implemented

conditional_on:
  - parent_decision_id: uncertainty_quantification
    influence_strength: strong
    conditional_table:
      - given_parent_option: temperature_scaling
        then_probabilities:
          automated_retrain: 0.20
          alert_human: 0.40
          gradual_rollback: 0.20
          none: 0.20
      - given_parent_option: conformal_prediction
        then_probabilities:
          automated_retrain: 0.30
          alert_human: 0.35
          gradual_rollback: 0.25
          none: 0.10
      - given_parent_option: mc_dropout
        then_probabilities:
          automated_retrain: 0.25
          alert_human: 0.35
          gradual_rollback: 0.20
          none: 0.20
      - given_parent_option: deep_ensembles
        then_probabilities:
          automated_retrain: 0.30
          alert_human: 0.30
          gradual_rollback: 0.25
          none: 0.15
      - given_parent_option: evidential
        # EDL deprecated (Phase 14) — minimal weight, redistributed to other rows
        then_probabilities:
          automated_retrain: 0.25
          alert_human: 0.35
          gradual_rollback: 0.20
          none: 0.20

  - parent_decision_id: data_validation_tools
    influence_strength: moderate
    conditional_table:
      - given_parent_option: pydantic_pandera
        then_probabilities:
          automated_retrain: 0.25
          alert_human: 0.35
          gradual_rollback: 0.20
          none: 0.20
      - given_parent_option: great_expectations
        then_probabilities:
          automated_retrain: 0.30
          alert_human: 0.30
          gradual_rollback: 0.20
          none: 0.20
      - given_parent_option: evidently
        then_probabilities:
          automated_retrain: 0.30
          alert_human: 0.35
          gradual_rollback: 0.20
          none: 0.15
      - given_parent_option: deepchecks
        then_probabilities:
          automated_retrain: 0.30
          alert_human: 0.30
          gradual_rollback: 0.25
          none: 0.15

archetype_weights:
  solo_researcher:
    probability_overrides:
      automated_retrain: 0.15
      alert_human: 0.45
      gradual_rollback: 0.10
      none: 0.30
    rationale: "Solo researchers manually review drift; automated pipelines are overkill"
  lab_group:
    probability_overrides:
      automated_retrain: 0.35
      alert_human: 0.30
      gradual_rollback: 0.20
      none: 0.15
    rationale: "Lab groups can invest in automated retraining pipelines"
  clinical_deployment:
    probability_overrides:
      automated_retrain: 0.40
      alert_human: 0.25
      gradual_rollback: 0.25
      none: 0.10
    rationale: "Clinical systems need automated drift response for patient safety"

volatility:
  classification: shifting
  last_assessed: 2026-02-23
  next_review: 2026-05-23
  change_drivers:
    - "Evidently drift detection maturity"
    - "whylogs profiling integration depth"
    - "Production deployment timeline"
    - "Regulatory requirements for model monitoring"
    - "PPRM prediction-powered risk monitoring"
    - "CyclOps healthcare ML auditing"
    - "Clinical drift detection advances"
    - "Keyes et al. (2024) three principles for monitoring"
    - "Cook et al. (2026) Mayo Clinic 17-algorithm lessons"
    - "Challenger-champion model promotion pattern"

domain_applicability:
  vascular_segmentation: 1.0
  cardiac_imaging: 1.0
  neuroimaging: 1.0
  general_medical: 1.0

rationale: >
  Alert human and automated retrain are now equally weighted (0.30 each),
  reflecting growing evidence for automated approaches. Zhang et al. (2026)
  PPRM provides formal false alarm guarantees for semi-supervised drift
  detection, making automated retraining more trustworthy. Subasri et al.
  (2023) demonstrate that transfer learning effectively remediates
  cross-hospital distribution shifts, while continual learning addresses
  temporal drift. Krishnan et al. (2022) CyclOps offers a practical toolkit
  for healthcare ML auditing and drift detection. The prior for automated
  retrain is raised from 0.25 to 0.30, and alert human is reduced from 0.35
  to 0.30, as PPRM's formal guarantees reduce the need for human-in-the-loop
  drift assessment in well-characterized shift scenarios.

research_notes: >
  Zhang et al. (2026) PPRM introduces prediction-powered risk monitoring
  with semi-supervised drift detection and formal false alarm guarantees
  (Algorithm 1, Section 3), directly applicable to deployed segmentation
  models. Subasri et al. (2023) provide clinical evidence of unidirectional
  data shifts across hospitals, comparing transfer learning and continual
  learning as remediation strategies. Krishnan et al. (2022) CyclOps offers
  a comprehensive toolkit for healthcare ML auditing, drift detection, and
  fairness evaluation. Kim et al. (2025) outline strategies for monitoring
  clinical AI systems in production. Chakraborty et al. (2025) adapt
  financial risk metrics for AI model performance monitoring. Moreo et al.
  (2025) analyze interconnections between calibration, quantification, and
  accuracy prediction under distribution shift. Ackerman et al. (2021)
  present the Evidently framework for interactive ML monitoring and reporting.
  Phase 14 additions: Keyes et al. (2024) propose three principles for
  monitoring deployed AI: integrity, performance, and impact monitoring.
  Cook et al. (2026) report lessons from monitoring 17 radiology AI
  algorithms at Mayo Clinic, providing practical operational guidance.
  Challenger-champion pattern is the recommended model promotion approach:
  shadow scoring against locked test set before promotion. Full retraining
  is preferred over continual learning for datasets under ~50 volumes
  (EWC Fisher information is unreliable with ~20 samples). Flühmann et al.
  (2025) enable label-free confusion matrix estimation under distribution
  shift. Kiyasseh et al. (2024) provide a framework for evaluating clinical
  AI after deployment. Owens et al. (2025) highlight the responsibility
  vacuum in AI governance for healthcare.

references:
  - citation_key: zhang2026pprm
    relevance: >
      Prediction-Powered Risk Monitoring provides semi-supervised drift
      detection with formal false alarm guarantees. Directly applicable
      to deployed segmentation models.
    sections:
      - "Algorithm 1"
      - "Section 3"
    supports_options:
      - automated_retrain
      - alert_human

  - citation_key: subasri2023driftclinical
    relevance: >
      Demonstrates unidirectional data shifts across hospitals. Transfer
      learning for remediation, continual learning for temporal drift.
    supports_options:
      - automated_retrain
      - alert_human

  - citation_key: krishnan2022cyclops
    relevance: >
      CyclOps toolkit for healthcare ML auditing, drift detection, and
      fairness evaluation.
    supports_options:
      - automated_retrain
      - alert_human

  - citation_key: kim2025monitoring
    relevance: >
      Strategies for monitoring clinical AI systems in production.
    supports_options:
      - alert_human
      - automated_retrain

  - citation_key: chakraborty2025finrisk
    relevance: >
      Financial risk metrics adapted for AI model performance monitoring.
    supports_options:
      - automated_retrain
      - gradual_rollback

  - citation_key: moreo2025calibquant
    relevance: >
      Interconnections between calibration, quantification, and accuracy
      prediction under distribution shift.
    supports_options:
      - automated_retrain

  - citation_key: ackerman2021evidently
    relevance: >
      Evidently framework for interactive ML monitoring and reporting.
    supports_options:
      - alert_human

  - citation_key: keyes2024monitoring
    relevance: >
      Three principles for monitoring deployed AI: integrity (data quality),
      performance (model accuracy), and impact (downstream outcomes).
    supports_options:
      - automated_retrain
      - alert_human

  - citation_key: cook2026mayo
    relevance: >
      Lessons from monitoring 17 radiology AI algorithms at Mayo Clinic.
      Practical operational guidance for clinical deployment monitoring.
    supports_options:
      - automated_retrain
      - alert_human
      - gradual_rollback

  - citation_key: fluhmann2025labelfree
    relevance: >
      Label-free confusion matrix estimation under distribution shift,
      enabling drift response without ground-truth labels.
    supports_options:
      - automated_retrain

  - citation_key: kiyasseh2024sudo
    relevance: >
      Framework for evaluating clinical AI after deployment, supporting
      structured post-deployment monitoring.
    supports_options:
      - alert_human
      - automated_retrain

  - citation_key: owens2025vacuum
    relevance: >
      Responsibility vacuum in AI governance highlights need for clear
      drift response ownership and accountability.
    supports_options:
      - alert_human

  - citation_key: leest2025causal
    relevance: >
      Causal System Maps provide structured methodology for drift root-cause
      analysis: AQ1 (route), AQ2 (localise), AQ3 (externalise). Enables
      targeted drift response rather than blanket retraining.
    supports_options:
      - alert_human
      - automated_retrain

  - citation_key: babic2025postmarket
    relevance: >
      Only ~5% of cleared AI medical devices have reported adverse events,
      suggesting systematic underreporting. Motivates proactive drift response
      over passive incident reporting.
    supports_options:
      - automated_retrain
      - alert_human

  - citation_key: esr2025pms
    relevance: >
      ESR consensus: PMS for AI should be continuous and proactive based on
      systematic real-world performance data, not passive incident reporting.
    supports_options:
      - automated_retrain
      - alert_human

  - citation_key: feng2022chexstray
    relevance: >
      CheXstray/MMC+ multi-modal drift detection validated during COVID-19
      distribution shift — demonstrates real-world drift response triggers.
    supports_options:
      - automated_retrain
      - alert_human

  - citation_key: rabanser2024drift
    relevance: >
      Nature Communications: input data monitoring essential for drift response
      — model performance alone insufficient as trigger.
    supports_options:
      - automated_retrain
      - alert_human

  - citation_key: roschewitz2026datasetshift
    relevance: >
      Automatic dataset shift identification enabling safe deployment and
      timely drift response in medical imaging AI.
    supports_options:
      - automated_retrain
      - gradual_rollback

  - citation_key: medmlops2025
    relevance: >
      MedMLOps four-pillar framework including continuous monitoring/validation/
      retraining — closest published comparator to MinIVess drift response design.
    supports_options:
      - automated_retrain
      - alert_human

tags:
  - operations
  - drift
  - monitoring
  - safety-critical
  - partially-resolved
