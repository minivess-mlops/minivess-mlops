# Prefect Flow: Evaluation & Ensemble Planning

> **User prompt (verbatim, 2026-02-26)**

See the open issues that we would be doing in the Prefect Flow on Analysis (deployment with BentoML and Monai Deploy will be on another PR then). So we need to know to serve ML flow models, how to determine which is the best (.yaml config should define "the single best metric" out of the multi-metric checkpoints with the 0.5*clDice+0.5*MASD being the default) for registering models with tags (with correct environment "prod" and "staging" if someone actually has prod server on-prem or in cloud and then likes to keep developing a separate dev version on staging. We should know how to create flexibly different types of ensembles, and from the the current runs we can easily do 4 types of ensembles 1) ensemble all folds per loss using the single-best metric, 2) ensemble all folds and all losses for single-best metric, 3) ensemble all folds per loss using all the "best metrics", 4) ensemble all folds and all losses with all the "best metrics" (this being the most complete and brute force Deep Ensemble /home/petteri/Dropbox/github-personal/sci-llm-writer/biblio/biblio-vascular/lakshminarayanan-2017-deep-ensembles-uncertainty.md). As we have done CV we don't exactly now have a fixed validation set, and we used all of our minivess dataset for training, so thus we have only train from minivess, and we need to later download a 3rd party test set to determine how much does this actually generalize. Meanwhile you can create some "debug test set" (n=4, random volumes) from the minivess to serve as the debug dataset that we can use for debugging/development and testing the mechanics while waiting for actual test sets. And we should modify the test dataloader to be slightly unorthdox (or not found in your typical demo github projects) so that you have dataloaders_dict that has keys like "minivess_test", "ucl_2pm", "harvard_3pm" so that you easily can evaluate the generalization performance on various 3rd party datasets separately and see what kinds of datasets the net generalizes and what it does not! The dict could have a second nesting level as well if for example "harvard_3pm" dataset providers have classified their volumes somehow, e.g. "thin vessels only", "thin and thick vessels mixed in the same volume", "thick vessels only" which could be an interest methodological test for the segmentor network as your vanilla Dice loos wouls struggle with the mixtures and easily just learns how to segment thick vessels only as you get a higher overlap with the mask. We could implement similar type of hierarchical dataloader_dict for the used validation dataset as well used during the training? Get's further more complicated obviously how many model checkpoints we get, but we need to come up with a good management/schema of this complexity and not be afraid of it? E.g. if we had 3 external datasets and 3 classes per each dataset, and 6 different multi-metric losses we would have 54 .pth files (3*3*6) saved for each fold whcih is quite a lot disk space (well this should be mapped to some output artifact volume in the Docker case). This ensembling and evaluation of single .pth files from the MLflow experiment should use the same metrics right? the ensembling needs the custom Class for pyfunc but otherwise the evaluation of an ensemble and a single model should not be too different, right? The MLflow could get get quite messy though? Any good ideas of this? Probably MLflow should have as many experiments as we have Prefect Flows involved? i.e. "data_io", "modelling", "evaluation" experiments as the sheer number of different runs explodes from modelling to evaluation. Each of those 54 .pth files should we evaluated as their "own run" whereas in the "training mlflow experiment" every .pth file is hidden behind a single runName (    https://mlflow.org/docs/latest/ml/tracking/tracking-api/#run-management). Similarly each different way of ensembling should get their own run and have the data lineage of the modelling and data_io run so that you can easily sort the runs and see which run in the end produced the best metric (whatever column you prefer from all the mlflow.log_metric() / mlflow.log_metrics(), does this make sense to you? And by default the runs are sorted by thr 0.5*clDice+0.5*MASD compound metric given by Metrics Reloaded to us! Let's first save my prompt verbatim to /home/petteri/Dropbox/github-personal/minivess-mlops/docs/planning/prefect-flow-evaluation-and-ensemble-planning.md, and then plan with reviewer agents how to best achieve this described features until converging into an optimal executable plan, to be executed with out self-learning TDD skill! Create new Issues if needed, and then let's start closing all the Issues related to evaluation (deployment-related skills will be left to later Deployment PR then)
