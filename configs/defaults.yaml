# Base config: Update relevant keys from this with your "task config"
# Base exapmple (VISSL) see e.g. https://github.com/facebookresearch/vissl/blob/main/vissl/config/defaults.yaml
VERSION: 0.01alpha
NAME: 'base_config'

config:
  # ----------------------------------------------------------------------------------- #
  # GLOBAL DEFAULTS
  # ----------------------------------------------------------------------------------- #
  VERBOSE: False

  # ----------------------------------------------------------------------------------- #
  # DATA
  # ----------------------------------------------------------------------------------- #
  DATA:
    # Where to get the data files
    DATA_SOURCE:
      # To think about here how to define multiple datasets,
      # TOADD, i.e. how to create multiple val, and test subdictionaries
      DATASET_NAMES:
        - 'MINIVESS'
      # For batch inference of volumes in a folder
      FOLDER:
        PLACEHOLDER: 'placeholder'
        TRANSFORMS:
          TEST: 'NO_AUG'
        DATASET:
          NAME: 'MONAI_CACHEDATASET'
          # See https://docs.monai.io/en/stable/data.html#cachedataset
          MONAI_CACHEDATASET:
            CACHE_RATE: 0.0
            NUM_WORKERS: 1
      MINIVESS:
        FETCH_METHOD: 'DVC'
        FETCH_METHODS:
          DVC:
            datapath_base: 'data'
            version: 'placeholder'
          EBrains:
            # this was originally used for the data, but it is a bit cumbersome to use programmatically,
            # wtih authentication (registration) requiring extra hoops for non-academics
            DATA_INFO_URL: 'https://doi.org/10.25493/HPBE-YHK'
            DATA_DOWNLOAD_URL: 'https://data.kg.ebrains.eu/zip?container=https://data-proxy.ebrains.eu/api/v1/buckets/d-bf268b89-1420-476b-b428-b85a913eb523'
        SPLITS:
          NAME: 'RANDOM'
          RANDOM:
            SEED: 42
            TEST_VAL_RATIO: 0.1
            TRAIN_RATIO: 0.8
          # More reproducible way would be to write the splits to disk, and then the end-user/researcher could
          # decide to use the predefined list, or re-shuffle the split. Or keep the test set always the same
          # and reshuffle train/val?
          TXT_FILE:
            PLACEHOLDER: 0
        # with run_mode = 'debug', pick first n samples of the split to speed things up
        SUBSET:
          NAME: 'ALL_SAMPLES'
          ALL_SAMPLES:
            PLACEHOLDER_PARAM: 'placeholder'
          # Number of first samples in the dataset
          DEBUG:
            TRAIN: 2
            VAL: 2
            TEST: 1
        # Dataset object, see e.g. "2. Cache intermediate outcomes into persistent storage"
        # https://github.com/Project-MONAI/tutorials/blob/main/acceleration/fast_model_training_guide.md
        DATASET:
          NAME: 'MONAI_CACHEDATASET'
          # See https://docs.monai.io/en/stable/data.html#cachedataset
          MONAI_CACHEDATASET:
            CACHE_RATE: 1.0
            NUM_WORKERS: 1
          # The Vanilla one that does not cache anything, useful for example when
          # running some CI/CD tests on resource-constrained CPU instances, do not
          # use for actual training
          # NOTE! not all the downstream code works with this option, just use cache_rate: 0.0
#          MONAI_DATASET:
#            PLACEHOLDER_PARAM: 'placeholder'
        # See below for explanations, think of using some hierarchical .yamls if this
        # gets very heavy and you start having a lot of different augmentation schemes
        # see Hydra and "Configuring Experiments"
        # https://hydra.cc/docs/patterns/configuring_experiments/
        TRANSFORMS:
          # Not sure how to implement the best, if you would like to use specific parameters,
          # i.e. just specific for Minivess that would be different from standard "BASIC_AUG"
          # e.g. different blur, rotation, spatial transformation strengths
          TRAIN: 'BASIC_AUG'
          VAL: 'NO_AUG'
          TEST: 'NO_AUG'
    # Dataloader object
    DATALOADER:
      SKIP_DATALOADER: False
      NAME: 'DATALOADER_BASIC'
      # See e.g. "ThreadDataLoader vs. DataLoader"
      # https://github.com/Project-MONAI/tutorials/blob/main/acceleration/fast_model_training_guide.md
      DATALOADER_BASIC:
        PLACEHOLDER: 1
      THREAD_DATALOADER:
        PLACEHOLDER: 1
      TRAIN:
        BATCH_SZ: 1
        NUM_WORKERS: 1
      VAL:
        BATCH_SZ: 2
        NUM_WORKERS: 1
      TEST:
        BATCH_SZ: 2
        NUM_WORKERS: 1

  # ----------------------------------------------------------------------------------- #
  # MACHINE (cpu, gpu)
  # ----------------------------------------------------------------------------------- #
  MACHINE:
    DISTRIBUTED: False
    DEVICE: "gpu"

  # ----------------------------------------------------------------------------------- #
  # MODEL
  # ----------------------------------------------------------------------------------- #
  # Every model is now defined as an ensemble of models, and if you want to do a single
  # architecture with single repeat, it is just a special case on this ensemble
  MODEL:
    # Think of how to configure this in practice the most efficiently. The idea would be to have
    # the freedom to define your submodel as you would wish, as you would a "normal single model"
    # e.g. (https://arxiv.org/abs/2007.04206)
    #         does different augmentation for each submodel
    #      (https://proceedings.neurips.cc//paper/2020/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf)
    #         use different model architecture for each submodel
    META_MODEL:
      - 'Unet'
      - 'SegResNet'
    # These are your default setting for different architectures implemented to the pipeline
    # You could separate this into a different config file like "architectures.yaml" or something
    # https://github.com/ctpn/minivess/blob/main/minivess_2dunet.ipynbv (the original model in Jupyter demo)
    Unet_2D:
      spatial_dims: 2
      in_channels: 1
      out_channels: 1
      channels:
        - 4
        - 8
        - 16
#      channels:
#        - 16
#        - 32
#        - 64
#        - 128
#        - 256
      strides:
        - 2
        - 2
#        - 2
#        - 2
      num_res_units: 2
    # https://docs.monai.io/en/stable/networks.html#unet
    Unet:
      spatial_dims: 3
      in_channels: 1
      out_channels: 1
      channels:
        - 4
        - 8
        - 16
      strides:
        - 2
        - 2
      num_res_units: 2
    # https://docs.monai.io/en/stable/networks.html#segresnet
    SegResNet:
      spatial_dims: 3
      # Strictly speaking these depend on the dataset more than your model, so derive these from
      # the dataset, rather than here (even though these are input arguments to the model)
      in_channels: 1
      out_channels: 1


  # ----------------------------------------------------------------------------------- #
  # TRANSFORM SCHEMES
  # i.e. "BASIC_AUG" does a bunch of transformations (augmentations)
  # ----------------------------------------------------------------------------------- #
  TRANSFORMS:
    BASIC_AUG:
      param1: 'placeholder'
    # For Test/Validation splits:
    NO_AUG:
      param1: 'placeholder'

  # ----------------------------------------------------------------------------------- #
  # TRAINING
  # ----------------------------------------------------------------------------------- #
  TRAINING:
    # Have a look on the float16 vs bfloat16, and whether to for example include
    # HuggingFace's accelerate here, or what is the easiest?
    SKIP_TRAINING: False
    Unet_2D:
      PRECISION: 'AMP'
      NO_REPEATS: 1
      NUM_EPOCHS: 2
      LR: 0.0001 # 1e-4 FIXME! Parse scientifc notations from .yaml correctly
      LOSS:
        NAME: 'DiceLoss'
        PARAMS:
          sigmoid: True
      OPTIMIZER:
        NAME: 'Adam'
      SCHEDULER:
        NAME: null
    Unet:
      PRECISION: 'AMP'
      NO_REPEATS: 1
      NUM_EPOCHS: 2
      LR: 0.0001 # 1e-4 FIXME! Parse scientifc notations from .yaml correctly
      LOSS:
        NAME: 'DiceFocalLoss'
        PARAMS:
          placeholder: 0
      OPTIMIZER:
        NAME: 'Novograd'
      SCHEDULER:
        NAME: 'CosineAnnealingLR'
      #      AUGMENTATION:
      #        # Not sure how to implement the best, if you would like to use specific parameters,
      #        # i.e. just specific for Minivess that would be different from standard "BASIC_AUG"
      #        # e.g. different blur, rotation, spatial transformation strengths
      #        TRAIN: 'BASIC_AUG'
      #        VAL: 'NO_AUG'
      #        TEST: 'NO_AUG'
    SegResNet:
      PRECISION: 'AMP'
      NO_REPEATS: 1
      NUM_EPOCHS: 2
      LR: 0.0001 # 1e-4 FIXME! Parse scientifc notations from .yaml correctly
      LOSS:
        NAME: 'DiceFocalLoss'
      OPTIMIZER:
        NAME: 'Novograd'
      SCHEDULER:
        NAME: 'CosineAnnealingLR'
      # NOTE! These go to the dataloader definition, so you would have to dataloader
      #  per model architecture as well for this, TODO!
      #  atm using the static augmentation definition by the dataset
      #      AUGMENTATION:
      #        # Not sure how to implement the best, if you would like to use specific parameters,
      #        # i.e. just specific for Minivess that would be different from standard "BASIC_AUG"
      #        # e.g. different blur, rotation, spatial transformation strengths
      #        TRAIN: 'BASIC_AUG'
      #        VAL: 'NO_AUG'
      #        TEST: 'NO_AUG'


  # ----------------------------------------------------------------------------------- #
  # TRAINING (global)
  # ----------------------------------------------------------------------------------- #
  #  TRAINING_GLOBAL:
  #    METRICS:
  #      NAMES:
  #        - 'DiceMetric'
  #        - 'compute_hausdorff_distance'


  # ----------------------------------------------------------------------------------- #
  # VALIDATION
  # ----------------------------------------------------------------------------------- #
  VALIDATION:
    # i.e. how many "best models" you want to save, if you feel like you want to track the best
    # model based on Dice, on Hausdorff distance, metric X, you would have three models per dataset
    # and if you have Minivess and DatasetB on your VAL split, you would be saving and tracking 3 x 2 "best models"
    METRICS_TO_TRACK:
      - 'dice'
      #- 'dice_batch'
    # maybe there is a better way, but when saving the models, we need to know if larger or smaller value
    # is considered better, so as many entries as above, DICE score better when higher, distances then would be
    # better as smaller, as would a loss be
    METRICS_TO_TRACK_OPERATORS:
      - 'max'
      #- 'max'
    # Do not track change for these n epochs at all if your first epochs have noisy metrics
    NO_WARMUP_EPOCHS: 0
    # You could smooth the metric curve if again you get noisy / spurious metrics, and get those
    # best metrics in practice only from the end of the training when optimization has converged
    EMA_SMOOTH: False
    SAVE_FULL_MODEL: True
    VALIDATION_PARAMS:
      roi_size:
        - 64
        - 64
        - 8
      sw_batch_size: 4
      overlap: 0.6


  # ----------------------------------------------------------------------------------- #
  # VALIDATION_BEST
  # ----------------------------------------------------------------------------------- #
  # If you want to compute some heavier metrics like Hausdorff, for each repeat, otherwise
  # you just get the basic metrics like Dice logged. Most of the time you probably don't care for this
  # and you can save some time by not computing this again. But if you want to compare like Hausdorff
  # between the ensemble and the best repeat, enable this then
  VALIDATION_BEST:
    enable: False
    # The "VALIDATION" above is done after each epoch, or after n epochs. This can get a bit
    # expensive to done often if you for example want to compute Hausdorff distance.
    # This "VALIDATION_BEST" controls the inference done for best model after each repeat,
    # for ensemble of models, etc.
    METRICS:
      - 'dice'
      - 'hausdorff'
    METRICS_OPERATORS:
      # which is better, higher (max) or lower (min) value for this metric (see above)
      - 'max'
      - 'min'

  # ----------------------------------------------------------------------------------- #
  # ENSEMBLE
  # ----------------------------------------------------------------------------------- #
  ENSEMBLE:
    enable: True
    PARAMS:
      method: 'average'
      # what is the cutoff for prediction of foreground
      mask_threshold: 0.5

  # ----------------------------------------------------------------------------------- #
  # DATASETS
  # ----------------------------------------------------------------------------------- #
  # If you want to use the vanilla PyTorch dataloader, here are the basic params
  # e.g. easier to re-use 3rd party repos that you have downloded for quick testing
  DATASETS:
    PYTORCH:
      PLACEHOLDER: 1

  # ----------------------------------------------------------------------------------- #
  # DATALOADERS
  # ----------------------------------------------------------------------------------- #

  # ----------------------------------------------------------------------------------- #
  # LOSS FUNCTIONS
  # ----------------------------------------------------------------------------------- #
  LOSS:
    # https://github.com/Project-MONAI/tutorials/blob/main/acceleration/distributed_training/brats_training_ddp.py
    DiceFocalLoss:
      smooth_nr: 0.00001 # 1e-5 FIXME! Parse scientifc notations from .yaml correctly
      smooth_dr: 0.00001 # 1e-5 FIXME! Parse scientifc notations from .yaml correctly
      squared_pred: True
      to_onehot_y: False
      sigmoid: True
      batch: True

  # ----------------------------------------------------------------------------------- #
  # OPTIMIZERS
  # ----------------------------------------------------------------------------------- #
  OPTIMIZER:
    Novograd:
      PLACEHOLDER: 1

  # ----------------------------------------------------------------------------------- #
  # SCHEDULERS
  # ----------------------------------------------------------------------------------- #
  SCHEDULER:
    CosineAnnealingLR:
      PLACEHOLDER: 1

  # ----------------------------------------------------------------------------------- #
  # TESTING
  # ----------------------------------------------------------------------------------- #
  TESTING:
    DATA:
      # Data on local disk, shared cache, S3 bucket, etc.
      DUMMY:
        enable: False
    # Pytorch or MONAI dataset creation
    DATASET:
      # Add None to metadata
      debug_testing: False
    # For Pytorch/MONAI dataloaders
    DATALOADER:
      # Adds errors to data to test the dataloader error handling if you only have good data
      debug_testing: False
      DATA_VALIDITY:
        enable: True
    MODEL:
      WEIGHTWATCHER:
        enable: True
        # These need to be supported by the weightwatcher library
        analyze_cfg:
          min_evals: 50,
          max_evals: 500

  # ----------------------------------------------------------------------------------- #
  # LOGGING
  # ----------------------------------------------------------------------------------- #
  LOGGING:
    unique_hyperparam_name_with_hash: True
    # Define a "main metric" to reduce the clutter on your logging columns, as in the number that you
    # mainly want to see
    MAIN_METRIC:
      # What metric determines the model saving to disk (you could e.g. track best loss and best metric)
      # Giving you 2 "best models" per training repeat
      tracked_metric: 'dice'
      # Metric that you think is the most informative ranking the runs
      metric: 'dice'
      # The evaluation performance on which dataset you are interested in the most
      dataset: 'MINIVESS'
    TENSORBOARD:
      placeholder: 1
    MLFLOW:
      TRACKING:
        enable: True
        DATASET:
          # https://mlflow.org/docs/latest/python_api/mlflow.data.html
          placeholder: 0
      MODEL_REGISTRY:
        placeholder: 1
      # Params for testing that you can load models from model registry, and the metrics match
      # those obtained during training
      TEST_LOGGING:
        split: 'TEST'
        split_subset: 'MINIVESS'
        # Test that the output (i.e. metrics) are the same between the ones obtained
        # during the training, and the ones inferenced from MLflow registered models
        # Note! that this can be problematic with the segmentation inference as the
        #  inference in MONAI is not deterministic?
        CHECK_LOCAL:
          check_weights: True
        CHECK_MLFLOW_MODELS:
          check_weights: True
          repeat_level_output: False
          ensemble_level_output: True
        CHECK_MLFLOW_MODEL_REGISTRY:
          placeholder: null
    WANDB:
      enable: False
      s3: null

  # ----------------------------------------------------------------------------------- #
  # SERVICES
  # refactor to be a separate .yaml
  # ----------------------------------------------------------------------------------- #
  SERVICES:
    # These can be obviously very different for different orgs and different users
    MLFLOW:
      server_URI: 'https://dagshub.com/petteriTeikari/minivess_mlops.mlflow'
    DVC:
      repo_url: 'https://github.com/petteriTeikari/minivess_mlops'
    S3:
      dataset_bucket: 's3://minivessdataset'
      artifacts_bucket: 's3://minivess-artifacts'
