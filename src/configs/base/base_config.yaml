# Base config: Update relevant keys from this with your "task config"
# Base exapmple (VISSL) see e.g. https://github.com/facebookresearch/vissl/blob/main/vissl/config/defaults.yaml
defaults:
  - _self_
  # you must specify the base config you want to run
  - config: ??? # check later Hydra/OmegaConf part

VERSION: 0.01alpha
NAME: 'base_config'

config:
  # ----------------------------------------------------------------------------------- #
  # GLOBAL DEFAULTS
  # ----------------------------------------------------------------------------------- #
  VERBOSE: False

  # ----------------------------------------------------------------------------------- #
  # DATA
  # ----------------------------------------------------------------------------------- #
  DATA:
    # Where to get the data files
    DATA_SOURCE:
      # To think about here how to define multiple datasets,
      # TOADD, i.e. how to create multiple val, and test subdictionaries
      DATASET_NAME:
        - 'MINIVESS'
      MINIVESS:
        DATA_INFO_URL: 'https://doi.org/10.25493/HPBE-YHK'
        DATA_DOWNLOAD_URL: 'https://data.kg.ebrains.eu/zip?container=https://data-proxy.ebrains.eu/api/v1/buckets/d-bf268b89-1420-476b-b428-b85a913eb523'
        SPLITS:
          NAME: 'RANDOM'
          RANDOM:
            SEED: 42
            TEST_VAL_RATIO: 0.1
            TRAIN_RATIO: 0.8
          # More reproducible way would be to write the splits to disk, and then the end-user/researcher could
          # decide to use the predefined list, or re-shuffle the split. Or keep the test set always the same
          # and reshuffle train/val?
          TXT_FILE:
            PLACEHOLDER: 0
        # With no commit, we will just commit the downloaded dataset. Note! thet EBRAINS allow versioning too
        # and we are not tracking that here (as in having dvc commit change when versioning changes
        DVC_COMMIT: null
        # Dataset object, see e.g. "2. Cache intermediate outcomes into persistent storage"
        # https://github.com/Project-MONAI/tutorials/blob/main/acceleration/fast_model_training_guide.md
        DATASET:
          NAME: 'MONAI_CACHEDATASET'
          # See https://docs.monai.io/en/stable/data.html#cachedataset
          MONAI_CACHEDATASET:
            CACHE_RATE: 1.0
            NUM_WORKERS: 0
          # See below for explanations, think of using some hierarchical .yamls if this
          # gets very heavy and you start having a lot of different augmentation schemes
          # see Hydra and "Configuring Experiments"
          # https://hydra.cc/docs/patterns/configuring_experiments/
        TRANSFORMS:
          # Not sure how to implement the best, if you would like to use specific parameters,
          # i.e. just specific for Minivess that would be different from standard "BASIC_AUG"
          # e.g. different blur, rotation, spatial transformation strengths
          TRAIN: 'BASIC_AUG'
          VAL: 'NO_AUG'
          TEST: 'NO_AUG'
    # Dataloader object
    DATALOADER:
      NAME: 'DATALOADER_BASIC'
      # See e.g. "ThreadDataLoader vs. DataLoader"
      # https://github.com/Project-MONAI/tutorials/blob/main/acceleration/fast_model_training_guide.md
      DATALOADER_BASIC:
        PLACEHOLDER: 1
      THREAD_DATALOADER:
        PLACEHOLDER: 1
      TRAIN:
        BATCH_SZ: 2
        NUM_WORKERS: 1
      VAL:
        BATCH_SZ: 2
        NUM_WORKERS: 1
      TEST:
        BATCH_SZ: 2
        NUM_WORKERS: 1

  # ----------------------------------------------------------------------------------- #
  # MACHINE (cpu, gpu)
  # ----------------------------------------------------------------------------------- #
  MACHINE:
    DISTRIBUTED: False
    DEVICE: "gpu"

  # ----------------------------------------------------------------------------------- #
  # MODEL
  # ----------------------------------------------------------------------------------- #
  MODEL:
    # default model. User can define their own model and use that instead.
    MODEL_NAME: 'SegResNet'
    Unet:
      spatial_dims: 3
      in_channels: 1
      out_channels: 1
      channels:
        - 16
        - 32
        - 64
        - 128
        - 256
      strides:
        - 2
        - 2
        - 2
        - 2
      num_res_units: 2
    SegResNet:
      spatial_dims: 3
      # Strictly speaking these depend on the dataset more than your model, so derive these from
      # the dataset, rather than here (even though these are input arguments to the model)
      in_channels: 1
      out_channels: 1


  # ----------------------------------------------------------------------------------- #
  # TRANSFORM SCHEMES
  # i.e. "BASIC_AUG" does a bunch of transformations (augmentations)
  # ----------------------------------------------------------------------------------- #
  TRANSFORMS:
    BASIC_AUG:
      param1: 'placeholder'
    NO_AUG:
      param1: 'placeholder'


  # ----------------------------------------------------------------------------------- #
  # TRAINING
  # ----------------------------------------------------------------------------------- #
  TRAINING:
    AMP: True
    NO_REPEATS: 2
    NUM_EPOCHS: 1
    LR: 0.0001 # 1e-4 FIXME! Parse scientifc notations from .yaml correctly
    LOSS:
      NAME: 'DiceFocalLoss'
    OPTIMIZER:
      NAME: 'Novograd'
    SCHEDULER:
      NAME: 'CosineAnnealingLR'
    METRICS:
      NAMES:
        - 'DiceMetric'
        - 'compute_hausdorff_distance'

  # ----------------------------------------------------------------------------------- #
  # VALIDATION
  # ----------------------------------------------------------------------------------- #
  VALIDATION:
    # i.e. how many "best models" you want to save, if you feel like you want to track the best
    # model based on Dice, on Hausdorff distance, metric X, you would have three models per dataset
    # and if you have Minivess and DatasetB on your VAL split, you would be saving and tracking 3 x 2 "best models"
    METRICS_TO_TRACK:
      - 'dice'
      #- 'dice_batch'
    # maybe there is a better way, but when saving the models, we need to know if larger or smaller value
    # is considered better, so as many entries as above, DICE score better when higher, distances then would be
    # better as smaller, as would a loss be
    METRICS_TO_TRACK_OPERATORS:
      - 'max'
      #- 'max'
    # Do not track change for these n epochs at all if your first epochs have noisy metrics
    NO_WARMUP_EPOCHS: 0
    # You could smooth the metric curve if again you get noisy / spurious metrics, and get those
    # best metrics in practice only from the end of the training when optimization has converged
    EMA_SMOOTH: False
    SAVE_FULL_MODEL: True

  # ----------------------------------------------------------------------------------- #
  # VALIDATION_BEST
  # ----------------------------------------------------------------------------------- #
  VALIDATION_BEST:
    # The "VALIDATION" above is done after each epoch, or after n epochs. This can get a bit
    # expensive to done often if you for example want to compute Hausdorff distance.
    # This "VALIDATION_BEST" controls the inference done for best model after each repeat,
    # for ensemble of models, etc.
    METRICS:
      - 'dice'
      - 'hausdorff'
    METRICS_OPERATORS:
      # which is better, higher (max) or lower (min) value for this metric (see above)
      - 'max'
      - 'min'

  # ----------------------------------------------------------------------------------- #
  # ENSEMBLE
  # ----------------------------------------------------------------------------------- #
  ENSEMBLE:
    enable: False

  # ----------------------------------------------------------------------------------- #
  # DATASETS
  # ----------------------------------------------------------------------------------- #
  # If you want to use the vanilla PyTorch dataloader, here are the basic params
  # e.g. easier to re-use 3rd party repos that you have downloded for quick testing
  DATASETS:
    PYTORCH:
      PLACEHOLDER: 1

  # ----------------------------------------------------------------------------------- #
  # DATALOADERS
  # ----------------------------------------------------------------------------------- #

  # ----------------------------------------------------------------------------------- #
  # LOSS FUNCTIONS
  # ----------------------------------------------------------------------------------- #
  LOSS_FUNCTIONS:
    # https://github.com/Project-MONAI/tutorials/blob/main/acceleration/distributed_training/brats_training_ddp.py
    DiceFocalLoss:
      smooth_nr: 0.00001 # 1e-5 FIXME! Parse scientifc notations from .yaml correctly
      smooth_dr: 0.00001 # 1e-5 FIXME! Parse scientifc notations from .yaml correctly
      squared_pred: True
      to_onehot_y: False
      sigmoid: True
      batch: True

  # ----------------------------------------------------------------------------------- #
  # OPTIMIZERS
  # ----------------------------------------------------------------------------------- #
  OPTIMIZERS:
    Novograd:
      PLACEHOLDER: 1

  # ----------------------------------------------------------------------------------- #
  # SCHEDULERS
  # ----------------------------------------------------------------------------------- #
  SCHEDULERS:
    CosineAnnealingLR:
      PLACEHOLDER: 1

  # ----------------------------------------------------------------------------------- #
  # LOGGING
  # ----------------------------------------------------------------------------------- #
  LOGGING:
    unique_hyperparam_name_with_hash: True
    TENSORBOARD:
      placeholder: 1
    MLFLOW:
      server_URI: 'https://dagshub.com/petteriTeikari/minivess_mlops.mlflow'
      TRACKING:
        enable: True

    WANDB:
      placeholder: 1
      s3: null