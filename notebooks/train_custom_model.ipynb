{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petteriTeikari/minivess_mlops/blob/main/notebooks/train_custom_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use a custom model quickly\n",
        "\n",
        "Reuse the data(loader) from the \"ML pipe\""
      ],
      "metadata": {
        "collapsed": false,
        "id": "7dba488f6c9898c9"
      },
      "id": "7dba488f6c9898c9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the libraries needed"
      ],
      "metadata": {
        "collapsed": false,
        "id": "b8e42e792c194f5e"
      },
      "id": "b8e42e792c194f5e"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "source": [
        "import sys\n",
        "import os"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-25T01:30:59.730637113Z",
          "start_time": "2023-10-25T01:30:59.686319664Z"
        },
        "id": "c200b12aa14f2287"
      },
      "id": "c200b12aa14f2287"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you run this Notebook on Colab, you need to install the Virtual Environment with Poetry yourself (what I understood):"
      ],
      "metadata": {
        "collapsed": false,
        "id": "df06e624709b6250"
      },
      "id": "df06e624709b6250"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "source": [
        "running_in_colab = 'google.colab' in str(get_ipython())\n",
        "if running_in_colab:\n",
        "    print('You are running this on COLAB so installing the environment here')\n",
        "    # TODO! Colab by default uses Python 3.10 so force the 3.8.17 here?\n",
        "    os.chdir(\"/content\")\n",
        "    !git clone https://github.com/petteriTeikari/minivess_mlops.git\n",
        "    !pip install poetry\n",
        "    os.chdir(\"/content/minivess_mlops\")\n",
        "    !poetry config virtualenvs.in-project true\n",
        "    # Running \"!poetry install --no-ansi\" is needed or not?\n",
        "    !poetry install\n",
        "    #!poetry shell\n",
        "    # https://stackoverflow.com/a/65440080/6412152\n",
        "else:\n",
        "    print('Assuming that you are runnign this from IDE,\\n'\n",
        "          'or some other environment where you have your Jupyter kernel created from Poetry files')"
      ],
      "metadata": {
        "id": "23525ddc876b758d",
        "ExecuteTime": {
          "end_time": "2023-10-25T14:31:35.277891306Z",
          "start_time": "2023-10-25T14:31:35.274960356Z"
        }
      },
      "id": "23525ddc876b758d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the other modules now available from Poetry environment and from the Github repo (see [in_Google_Colab.ipynb](https://github.com/elise-chin/poetry-and-colab/blob/main/Using_python_poetry_in_Google_Colab.ipynb))"
      ],
      "metadata": {
        "collapsed": false,
        "id": "8986f81887cf72c4"
      },
      "id": "8986f81887cf72c4"
    },
    {
      "cell_type": "code",
      "source": [
        "!poetry show -v # /content/minivess_mlops/.venv"
      ],
      "metadata": {
        "id": "kuyBmlId3cLq"
      },
      "id": "kuyBmlId3cLq",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "source": [
        "if running_in_colab:\n",
        "  VENV_BASE = '/content/minivess_mlops/.venv'\n",
        "  VENV_PATH = os.path.join(VENV_BASE, 'lib', 'python3.10', 'site-packages')\n",
        "  #VENV_PATH = \"/content/gdrive/MyDrive/test-poetry/.venv/lib/python3.8/site-packages\"\n",
        "  #!mkdir /content/venv\n",
        "  LOCAL_VENV_PATH = '/content/venv_poetry' # local notebook\n",
        "  try:\n",
        "    os.symlink(VENV_PATH, LOCAL_VENV_PATH) # connect to directory in drive\n",
        "  except Exception as e:\n",
        "    print('symlink failed, e = {}'.format(e))\n",
        "  sys.path.insert(0, LOCAL_VENV_PATH)\n",
        "\n",
        "  sys.path.insert(0,'/content/minivess_mlops')"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-25T01:31:02.499068266Z",
          "start_time": "2023-10-25T01:31:02.486576033Z"
        },
        "id": "18c3a271cdfbd652"
      },
      "id": "18c3a271cdfbd652"
    },
    {
      "cell_type": "code",
      "source": [
        "from loguru import logger\n",
        "\n",
        "from src.run_training import parse_args_to_dict\n",
        "from src.training.experiment import define_experiment_data\n",
        "from src.utils.config_utils import import_config"
      ],
      "metadata": {
        "id": "zUYqIvC5o7As"
      },
      "id": "zUYqIvC5o7As",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import helper subfunction(s)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "9aed4aabb6c2f770"
      },
      "id": "9aed4aabb6c2f770"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "source": [
        "def get_dataloaders(experim_dataloaders: dict):\n",
        "    # Get the \"validation\" and \"train\" dataloaders from the dictionary\n",
        "    fold_name = 'fold0'\n",
        "    split_names = list(experim_dataloaders[fold_name].keys())\n",
        "    fold_key = experim_dataloaders.get(fold_name)\n",
        "    if fold_key is not None:\n",
        "        try:\n",
        "            train = experim_dataloaders[fold_name]['TRAIN']\n",
        "            val = experim_dataloaders[fold_name]['VAL']['MINIVESS']\n",
        "        except Exception as e:\n",
        "            raise IOError('Could not get the dataloaders from the dictionary, error = {}'.format(e))\n",
        "    else:\n",
        "        raise IOError('Fold name = \"{}\" not found in the dataloaders dictionary'.format(fold_name))\n",
        "    return train, val"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-25T01:31:07.120380382Z",
          "start_time": "2023-10-25T01:31:07.114209819Z"
        },
        "id": "3e05f12233738a40"
      },
      "id": "3e05f12233738a40"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input arguments for the training (you can add all the input arguments supported by `run_training.py` here"
      ],
      "metadata": {
        "collapsed": false,
        "id": "285ff9d623a96c9f"
      },
      "id": "285ff9d623a96c9f"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "source": [
        "input_args = ['-c', 'tutorials/train_demo']\n",
        "\n",
        "# Fake these as coming from the command line to match the main code (run_training.py)\n",
        "sys.argv = ['notebook_run']  # Jupyter has all the extra crap, so replace that with this\n",
        "for sysargv in input_args:\n",
        "    sys.argv.append(sysargv)\n",
        "args = parse_args_to_dict()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-25T01:31:08.972102213Z",
          "start_time": "2023-10-25T01:31:08.946666458Z"
        },
        "id": "cfe20cf4816832a7"
      },
      "id": "cfe20cf4816832a7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the config with Hydra from the .yaml file(s)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "91f922a0c5e5c340"
      },
      "id": "91f922a0c5e5c340"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "source": [
        "from src.utils.config_utils import config_import_script, set_up_experiment_run\n",
        "\n",
        "config = config_import_script(args['task_config_file'],\n",
        "                              parent_dir_string = '.', #f'..{os.sep}..',\n",
        "                              parent_dir_string_defaults = f'..{os.sep}..')\n",
        "print(config)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-25T01:31:10.428700721Z",
          "start_time": "2023-10-25T01:31:10.169865984Z"
        },
        "id": "c432cc673828082d"
      },
      "id": "c432cc673828082d"
    },
    {
      "cell_type": "code",
      "source": [
        "if running_in_colab:\n",
        "  os.makedirs('volumes', exist_ok=True)\n",
        "\n",
        "  data_dir = os.path.join('volumes', 'minivess-dvc-cache')\n",
        "  os.makedirs(data_dir, exist_ok=True)\n",
        "  args['data_dir'] = data_dir\n",
        "\n",
        "  arfifacts_dir = os.path.join('volumes', 'minivess-artifacts')\n",
        "  os.makedirs(arfifacts_dir, exist_ok=True)\n",
        "  args['output_dir'] = arfifacts_dir"
      ],
      "metadata": {
        "id": "_YUkKC8HAGpD"
      },
      "id": "_YUkKC8HAGpD",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_run = set_up_experiment_run(config=config,\n",
        "                                args=args,\n",
        "                                log_level='DEBUG')\n",
        "print(exp_run)"
      ],
      "metadata": {
        "id": "CVXYyW7yAGW0"
      },
      "id": "CVXYyW7yAGW0",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the dataloaders (now the data augmentations are here as well as data transformations)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "40c67cdc100d7ebc"
      },
      "id": "40c67cdc100d7ebc"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-fea1dbeb4d72>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m _, _, experim_dataloaders, exp_run = (\n\u001b[0;32m----> 2\u001b[0;31m         define_experiment_data(config=config,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                exp_run=exp_run))\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Get the \"validation\" and \"train\" dataloaders from the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/minivess_mlops/src/training/experiment.py\u001b[0m in \u001b[0;36mdefine_experiment_data\u001b[0;34m(config, exp_run)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Collect the data and define splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mfold_split_file_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_run\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         import_datasets(data_config=config['config']['DATA'],\n\u001b[0m\u001b[1;32m     31\u001b[0m                         \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_run\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ARGS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mrun_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_run\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ARGS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'run_mode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/minivess_mlops/src/utils/data_utils.py\u001b[0m in \u001b[0;36mimport_datasets\u001b[0;34m(data_config, data_dir, config, exp_run, run_mode)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets_to_import\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mdataset_filelistings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_split_file_dicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_stats\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             import_dataset(data_config=data_config,\n\u001b[0m\u001b[1;32m     34\u001b[0m                            \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                            \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/minivess_mlops/src/utils/data_utils.py\u001b[0m in \u001b[0;36mimport_dataset\u001b[0;34m(data_config, data_dir, dataset_name, config, exp_run, run_mode)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'MINIVESS'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mfilelisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_split_file_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_stats\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             = import_minivess_dataset(dataset_cfg=dataset_cfg,\n\u001b[0m\u001b[1;32m     69\u001b[0m                                       \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                                       \u001b[0mrun_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/minivess_mlops/src/datasets/minivess.py\u001b[0m in \u001b[0;36mimport_minivess_dataset\u001b[0;34m(dataset_cfg, data_dir, run_mode, config, exp_run, dataset_name, fetch_method, fetch_params)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfetch_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DVC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         dataset_dir = fetch_dataset_with_dvc(fetch_params=fetch_params,\n\u001b[0m\u001b[1;32m     28\u001b[0m                                              \u001b[0mdataset_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                              \u001b[0mdataset_name_lowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/minivess_mlops/src/datasets/minivess.py\u001b[0m in \u001b[0;36mfetch_dataset_with_dvc\u001b[0;34m(fetch_params, dataset_cfg, dataset_name_lowercase, repo_url, repo_dir)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# TODO! This is now based on \"dvc pull\" by Github Actions or manual, but you could try\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m#  to get the Python programmatic API to work too (or have \"dvc pull\" from subprocess)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     dataset_dir = get_dvc_files_of_repo(repo_dir=repo_dir,\n\u001b[0m\u001b[1;32m     70\u001b[0m                                         \u001b[0mrepo_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                                         \u001b[0mdataset_name_lowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name_lowercase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/minivess_mlops/src/datasets/dvc_utils.py\u001b[0m in \u001b[0;36mget_dvc_files_of_repo\u001b[0;34m(repo_dir, repo_url, dataset_name_lowercase, fetch_params, dataset_cfg, local_download_duplicate, use_local_repo, skip_already_downloaded, use_fs_check)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Problem getting the DVC cache dir from DVC path \"{}\", e = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvc_repo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcache_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mcheck_for_dvc_cache_existence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/minivess_mlops/src/datasets/dvc_utils.py\u001b[0m in \u001b[0;36mcheck_for_dvc_cache_existence\u001b[0;34m(cache_dir)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_for_dvc_cache_existence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DVC cache dir does not exist in \"{}\"'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
          ]
        }
      ],
      "source": [
        "_, _, experim_dataloaders, exp_run = (\n",
        "        define_experiment_data(config=config,\n",
        "                               exp_run=exp_run))\n",
        "\n",
        "# Get the \"validation\" and \"train\" dataloaders from the dictionary\n",
        "train, val = get_dataloaders(experim_dataloaders)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-25T01:26:37.985819948Z",
          "start_time": "2023-10-25T01:26:37.650394408Z"
        },
        "id": "e231509f5afb185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "3df76fce-3e4b-43df-e5eb-a8806636b572"
      },
      "id": "e231509f5afb185"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you are ready to train your new model that you just wanna quickly test without\n",
        "wanting to have a battle with the config .YAML files\n",
        "\n",
        "Add maybe some fastai demo with MLflow autologging:\n",
        "[https://github.com/mlflow/mlflow/blob/master/examples/fastai/train.py](https://github.com/mlflow/mlflow/blob/master/examples/fastai/train.py)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "730a808f93390d1e"
      },
      "id": "730a808f93390d1e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Iterate the dataloaders for demo\n",
        "no_of_epochs = 3\n",
        "logger.info('Training for {} epochs'.format(no_of_epochs))\n",
        "for epoch in range(no_of_epochs):\n",
        "\n",
        "    logger.info('Epoch {}/{}'.format(epoch, no_of_epochs - 1))\n",
        "\n",
        "    # Train\n",
        "    logger.info('train with {} batches'.format(len(train)))\n",
        "    for i, batch in enumerate(train):\n",
        "        images, mask = batch['image'], batch['label']\n",
        "\n",
        "    # Validation\n",
        "    logger.info('validate with {} batches'.format(len(train)))\n",
        "    for j, batch in enumerate(val):\n",
        "        images, mask = batch['image'], batch['label']\n",
        "\n",
        "logger.info('Training done!')"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-25T01:28:33.785824734Z",
          "start_time": "2023-10-25T01:26:37.990647342Z"
        },
        "id": "ee4ad4c2e241e65b"
      },
      "id": "ee4ad4c2e241e65b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}