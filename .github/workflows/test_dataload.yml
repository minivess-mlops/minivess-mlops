name: Test (Dataload)

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: checkout repo content
        uses: actions/checkout@v2 # checkout the repository content
      - name: dockerhub login
        env:
          DOCKERHUB_USER: ${{secrets.DOCKERHUB_USERNAME}}
          DOCKERHUB_PASSWORD: ${{secrets.DOCKERHUB_PASSWORD}}
        run:
          docker login -u $DOCKERHUB_USER -p $DOCKERHUB_PASSWORD
      - name: pull Docker image
        run: docker pull petteriteikari/minivess-mlops-train:latest
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{secrets.AWS_ACCESS_KEY_ID}}
          aws-secret-access-key: ${{secrets.AWS_SECRET_ACCESS_KEY}}
          aws-region: eu-north-1
      - name: Install s3fs
        run: sudo apt update && sudo apt-get install s3fs && which s3fs
      - name: Set up ".passwd-s3fs"
        run: echo ${{secrets.AWS_ACCESS_KEY_ID}}:${{secrets.AWS_SECRET_ACCESS_KEY}} \
          > ${HOME}/.passwd-s3fs && chmod 600 ${HOME}/.passwd-s3fs
      - name: Create directories for mounting S3 volumes
        run: sudo mkdir ${HOME}/minivess-dvc-cache && \ 
          sudo mkdir ${HOME}/minivess-artifacts && \
          sudo chmod 777 ${HOME}/minivess-dvc-cache && \
          sudo chmod 777 ${HOME}/minivess-artifacts
      - name: Mount S3 data volume (DVC Shared Cache) to a local instance
        # to upgrade later with the Docker Compose?
        # https://stackoverflow.com/questions/49379881/mount-s3fs-as-docker-volume?rq=3
        run: s3fs minivess-dvc-cache ${HOME}/minivess-dvc-cache \ 
          -o passwd_file=${HOME}/.passwd-s3fs -o dbglevel=info -f -o curldbg
      - name: Mount S3 artifacts volume to a local instance
        # to upgrade later with the Docker Compose?
        # https://stackoverflow.com/questions/49379881/mount-s3fs-as-docker-volume?rq=3
        run: s3fs minivess-artifacts ${HOME}/minivess-artifacts \ 
          -o passwd_file=${HOME}/.passwd-s3fs -o dbglevel=info -f -o curldbg
      # https://towardsdatascience.com/dvc-github-actions-automatically-rerun-modified-components-of-a-pipeline-a3632519dc42
      - name: Check diskspace before DVC pull
        run: sudo df -h
      - name: Pull data from DVC
        run:
          # dvc remote modify origin --local auth basic
          dvc remote modify remote_storage access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          dvc remote modify remote_storage secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          dvc pull
      - name: Check diskspace after DVC pull
        run: sudo df -h
      - name: Execute "run_training.py" for testing data loading
        env:
          MLFLOW_USER: ${{secrets.MLFLOW_USERNAME}}
          MLFLOW_PASSWORD: ${{secrets.MLFLOW_PASSWORD}}
        run: >
          docker run --rm \ 
          -v ${HOME}/minivess-artifacts:/mnt/minivess-artifacts \ 
          -v ${HOME}/minivess-dvc-cache:/mnt/minivess-dvc-cache \
          petteriteikari/minivess-mlops-train:latest \
          python run_training.py \ 
          -c task_config.yaml -run test_dataload # Pythion Input arguments

